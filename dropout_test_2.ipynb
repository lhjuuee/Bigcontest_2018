{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model management\n",
    "## Saver\n",
    "After training model, if we want to reuse model, we need to save model parameters. And if we want to continue training,\n",
    "\n",
    "defining checkpoint would be really helpful. Checkpoint make us be able to start from last checkpoint.\n",
    "\n",
    "Let's get into it.\n",
    "\n",
    "##### In TensorFlow\n",
    "\n",
    "In, **setup phase**, all we need to do is, after completing nodes at last part of setup, add saver node and\n",
    "\n",
    "in, **run phase**, if you want to save model call save() method by delivering session and checkpoint path!\n",
    "\n",
    "##### Example is bigcontest data\n",
    "### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 4)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "train_activity_orig = pd.read_csv('C:/Bigcontest/train_activity.csv')\n",
    "train_label_orig = pd.read_csv('C:/Bigcontest/train_label.csv')\n",
    "\n",
    "train_activity_mean_orig = train_activity_orig.groupby(['acc_id'], as_index=False).mean()\n",
    "\n",
    "train_activity_label_orig = pd.merge(train_activity_mean_orig, train_label_orig, how='outer', on='acc_id')\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=10000, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(train_activity_label_orig, train_activity_label_orig['label']):\n",
    "    strat_train_dev_set_orig = train_activity_label_orig.loc[train_index]\n",
    "    strat_test_set_orig = train_activity_label_orig.loc[test_index]\n",
    "\n",
    "strat_train_dev_set_orig = strat_train_dev_set_orig.reset_index(drop=True)\n",
    "\n",
    "split2 = StratifiedShuffleSplit(n_splits=1, test_size=20000)\n",
    "\n",
    "for train_index, test_index in split2.split(strat_train_dev_set_orig, strat_train_dev_set_orig['label']):\n",
    "    strat_train_set_orig = strat_train_dev_set_orig.loc[train_index]\n",
    "    strat_dev_set_orig = strat_train_dev_set_orig.loc[test_index]\n",
    "\n",
    "label_mapping = {\"week\": 0, \"month\": 1, \"2month\": 2, \"retained\": 3}\n",
    "\n",
    "for dataset in (strat_train_set_orig, strat_dev_set_orig, strat_test_set_orig):\n",
    "    dataset['label'] = dataset['label'].map(label_mapping)\n",
    "\n",
    "X_train_orig = strat_train_set_orig.drop(['acc_id', 'label'], axis=1)\n",
    "X_dev_orig = strat_dev_set_orig.drop(['acc_id', 'label'], axis=1)\n",
    "X_test_orig = strat_test_set_orig.drop(['acc_id', 'label'], axis=1)\n",
    "\n",
    "Y_train_orig = np.array(strat_train_set_orig['label']).reshape(-1,1)\n",
    "Y_dev_orig = np.array(strat_dev_set_orig['label']).reshape(-1,1)\n",
    "Y_test_orig = np.array(strat_test_set_orig['label']).reshape(-1,1)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "Y_train_orig_oh = ohe.fit_transform(Y_train_orig).toarray()\n",
    "Y_dev_orig_oh = ohe.fit_transform(Y_dev_orig).toarray()\n",
    "Y_test_orig_oh = ohe.fit_transform(Y_test_orig).toarray()\n",
    "\n",
    "X_train = X_train_orig.T\n",
    "X_dev = X_dev_orig.T\n",
    "X_test = X_test_orig.T\n",
    "\n",
    "Y_train = Y_train_orig_oh.T\n",
    "Y_dev = Y_dev_orig_oh.T\n",
    "Y_test = Y_test_orig_oh.T\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_dev = np.array(X_dev)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "X_train_orig = np.array(X_train_orig)\n",
    "X_dev_orig = np.array(X_dev_orig)\n",
    "X_test_orig = np.array(X_test_orig)\n",
    "\n",
    "Y_train_orig_oh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() \n",
    "\n",
    "n_inputs = 37\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 200\n",
    "n_hidden3 = 100\n",
    "n_hidden4 = 50\n",
    "\n",
    "n_outputs = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "Y = tf.placeholder(tf.int64, shape=(None), name=\"Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 drop out and batch-normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = tf.placeholder_with_default(True, shape=(), name=\"training\")\n",
    "\n",
    "dropout_rate = 0.5\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    \n",
    "    my_batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
    "    \n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = my_batch_norm_layer(hidden1)\n",
    "    bn1_act = tf.nn.elu(bn1)\n",
    "    dropout1 = tf.layers.dropout(inputs=bn1_act, rate = dropout_rate, training=training)\n",
    "    \n",
    "    hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = my_batch_norm_layer(hidden2)\n",
    "    bn2_act = tf.nn.elu(bn2)\n",
    "    dropout2 = tf.layers.dropout(inputs=bn2_act, rate = dropout_rate, training=training)\n",
    "    \n",
    "    hidden3 = tf.layers.dense(bn2_act, n_hidden3, name=\"hidden3\")\n",
    "    bn3 = my_batch_norm_layer(hidden3)\n",
    "    bn3_act = tf.nn.elu(bn3)\n",
    "    dropout3 = tf.layers.dropout(inputs=bn3_act, rate = dropout_rate, training=training)\n",
    "    \n",
    "    hidden4 = tf.layers.dense(bn3_act, n_hidden4, name=\"hidden4\")\n",
    "    bn4 = my_batch_norm_layer(hidden4)\n",
    "    bn4_act = tf.nn.elu(bn4)\n",
    "    dropout4 = tf.layers.dropout(inputs=bn4_act, rate = dropout_rate, training=training)\n",
    "    \n",
    "    logits_before_bn = tf.layers.dense(dropout4, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 compute loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-3db26f16e48d>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    \n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    \n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate = learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 evaluation\n",
    "\n",
    "tf.argmax(y_, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, tf.argmax(Y,1), 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X_train, Y_train, batch_size):\n",
    "    \n",
    "    rnd_indices = np.random.randint(0, len(X_train_orig), batch_size)\n",
    "    X_batch = X_train[rnd_indices, :]\n",
    "    Y_batch = Y_train[rnd_indices, :]\n",
    "    \n",
    "    return X_batch, Y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 drop out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.3\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saver!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create log directory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom datetime import datetime\\n\\ndef log_dir(prefix=\"\"):\\n    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\\n    root_logdir = \"tf_logs\"\\n    if prefix:\\n        prefix += \"-\"\\n    name = prefix + \"run-\" + now\\n    return \"{}/{}/\".format(root_logdir, name)\\n\\nlogdir = log_dir(\"savetest\")\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)\n",
    "\n",
    "logdir = log_dir(\"savetest\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summmary = tf.summary.scalar('LOSS', loss)\n",
    "#file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10000\n",
    "batch_size = 32\n",
    "m = 70000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paused training. Continue epoch. 3756\n",
      "INFO:tensorflow:Restoring parameters from /tmp/do_test_2.ckpt\n",
      "Cost after epoch 3760: 0.636345\n",
      "Train accuracy: 0.72525716 \tDev accuracy 0.7118\n"
     ]
    }
   ],
   "source": [
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "checkpoint_path = \"/tmp/do_test_2.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./final_do_test_2\"\n",
    "\n",
    "seed = 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Paused training. Continue epoch.\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "        \n",
    "    \n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        \n",
    "        num_minibatches = int(m / batch_size)\n",
    "        epoch_cost = 0\n",
    "        seed = seed +1\n",
    "            \n",
    "        for iteration in range(int(np.ceil(m / batch_size))):\n",
    "            X_batch, Y_batch = random_batch(X_train_orig, Y_train_orig_oh, batch_size)\n",
    "            _, minibatch_cost,_ = sess.run([training_op, loss, extra_update_ops], feed_dict={training: True, X: X_batch, Y: Y_batch})\n",
    "            epoch_cost += minibatch_cost / num_minibatches\n",
    "            \n",
    "            \n",
    "        if epoch % 5 == 0:\n",
    "            \n",
    "            print(\"Cost after epoch %i: %f\" %(epoch, epoch_cost))\n",
    "           \n",
    "            acc_train = accuracy.eval(feed_dict={X: X_train_orig, Y: Y_train_orig_oh})\n",
    "            acc_dev = accuracy.eval(feed_dict={X: X_dev_orig, Y: Y_dev_orig_oh})\n",
    "            print(\"Train accuracy:\", acc_train, \"\\tDev accuracy\", acc_dev)\n",
    "            \n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "        \n",
    "    saver.save(sess, final_model_path + \".ckpt\") # Final checkpoint after completing    \n",
    "    os.remove(checkpoint_epoch_path)    \n",
    "    \n",
    "            \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look!\n",
    "\n",
    "carefully at those saver marks on the code above. \n",
    "\n",
    "## Reuse graph!\n",
    "\n",
    "Save method basically save its structure of graph in .meta file.\n",
    "\n",
    "So, by tf.train.import_meta_graph(), we can reuse graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "saver = tf.train.import_meta_graph(final_model_path+\".meta\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, final_model_path)\n",
    "    [...]\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
